{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de8cf44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ab31941",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"twitter_training.csv\",header=None,names=[\"id\",\"game\",\"rating\",\"comment\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33d655fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"id\",\"game\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "810dce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0cfc3da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Positive', 'Neutral', 'Negative', 'Irrelevant'], dtype=object)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"rating\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "924769e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "MODEL = f\"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff78501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment were not used when initializing TFRobertaModel: ['classifier']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_model_1/roberta/pooler/dense/kernel:0', 'tf_roberta_model_1/roberta/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-26 20:56:45.061016: I external/local_xla/xla/service/service.cc:163] XLA service 0x75f6ed87b5f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-10-26 20:56:45.061065: I external/local_xla/xla/service/service.cc:171]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 SUPER, Compute Capability 7.5\n",
      "2025-10-26 20:56:46.312552: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-10-26 20:56:47.236802: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:473] Loaded cuDNN version 91002\n",
      "I0000 00:00:1761501408.555637   10475 device_compiler.h:196] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14999/14999 [==============================] - 2550s 167ms/step - loss: 0.7739 - sparse_categorical_accuracy: 0.6990 - val_loss: 0.5198 - val_sparse_categorical_accuracy: 0.8173\n",
      "Epoch 2/3\n",
      "14999/14999 [==============================] - 2113s 141ms/step - loss: 0.3351 - sparse_categorical_accuracy: 0.8775 - val_loss: 0.3398 - val_sparse_categorical_accuracy: 0.8837\n",
      "Epoch 3/3\n",
      "14999/14999 [==============================] - 2076s 138ms/step - loss: 0.1989 - sparse_categorical_accuracy: 0.9258 - val_loss: 0.3527 - val_sparse_categorical_accuracy: 0.8961\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Functional' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     78\u001b[39m history = model.fit(train_dataset, validation_data=test_dataset, epochs=\u001b[32m3\u001b[39m)\n\u001b[32m     80\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# 7️⃣ Save fine-tuned model\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m./tf_sentiment_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     84\u001b[39m tokenizer.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33m./tf_sentiment_model\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     86\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ Memory-safe 4-class TensorFlow sentiment model saved\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'Functional' object has no attribute 'save_pretrained'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "label_map = {\"Negative\": 0, \"Neutral\": 1, \"Positive\": 2, \"Irrelevant\": 3}\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "train_texts = train_df['comment'].tolist()\n",
    "train_labels = [label_map[l] for l in train_df['rating']]\n",
    "\n",
    "test_texts = test_df['comment'].tolist()\n",
    "test_labels = [label_map[l] for l in test_df['rating']]\n",
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "max_len = 64  \n",
    "batch_size = 4\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_len, return_tensors='tf')\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_len, return_tensors='tf')\n",
    "\n",
    "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
    "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    ")).shuffle(1000).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    test_labels\n",
    ")).batch(batch_size)\n",
    "\n",
    "base_model = TFAutoModel.from_pretrained(model_name)\n",
    "num_labels = 4\n",
    "\n",
    "input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
    "attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "outputs = base_model(input_ids, attention_mask=attention_mask)[0]  \n",
    "logits = tf.keras.layers.Dense(num_labels, activation=None)(cls_token)\n",
    "model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=logits)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "history = model.fit(train_dataset, validation_data=test_dataset, epochs=3)\n",
    "\n",
    "model.sa(\"./tf_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./tf_sentiment_model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3e379b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e7bff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
